{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chidam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/chidam/Desktop/text classification_train set.csv')\n",
    "test = pd.read_csv('/Users/chidam/Desktop/text classification_test set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using train_test_split from the model_selection module of scikit-learn\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Basic Models\n",
    "\n",
    "#Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic \n",
    "#Regression\n",
    "\n",
    "\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17621x15102 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 198521 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.626 \n"
     ]
    }
   ],
   "source": [
    "# We have our first model with a multiclass logloss of 0.626.\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from \n",
    "# scikit-learn.\n",
    "\n",
    "\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.528 \n"
     ]
    }
   ],
   "source": [
    "Improved the first model above by 0.1!!!\n",
    "\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens when we use naive bayes on these two datasets:\n",
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "#good performance, But the logistic regression on counts is still better! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more ancient algorithms is SVMs.\n",
    "\n",
    "# Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM.\n",
    "\n",
    "# Before applying SVMs, we must standardize the data.\n",
    "\n",
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features with specified parameters\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features without specifying the parameters  while calling the classifier\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next on to technique for hyperparameter optimization: Grid Search\n",
    "    \n",
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   38.7s remaining:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   43.3s finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.740\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#The score comes similar to what we had for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    1.8s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    1.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is an improvement of 8% over the original naive bayes score!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [03:15, 9830.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# In NLP problems, it's customary to look at word vectors. Word vectors give a lot of insights about the data.\n",
    "\n",
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('/Users/chidam/Desktop/glove.42B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    \n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# embeddings_index = {}\n",
    "\n",
    "# f = open('glove.840B.300d.txt')\n",
    "\n",
    "# for line in tqdm(f):\n",
    "\n",
    "# values = line.split()\n",
    "\n",
    "# word = values[0]\n",
    "\n",
    "# # Catch the exception where there are strings in the Glove text file.\n",
    "\n",
    "# try:\n",
    "\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "#     embeddings_index[word] = coefs\n",
    "\n",
    "# except ValueError:\n",
    "#     pass\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/17621 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/17621 [00:00<35:29,  8.27it/s]\u001b[A\n",
      "  0%|          | 6/17621 [00:00<26:48, 10.95it/s]\u001b[A\n",
      "  0%|          | 17/17621 [00:00<19:38, 14.94it/s]\u001b[A\n",
      "  0%|          | 27/17621 [00:00<14:45, 19.87it/s]\u001b[A\n",
      "  0%|          | 45/17621 [00:00<10:48, 27.09it/s]\u001b[A\n",
      "  0%|          | 63/17621 [00:00<08:05, 36.17it/s]\u001b[A\n",
      "  0%|          | 75/17621 [00:00<06:25, 45.48it/s]\u001b[A\n",
      "  1%|          | 93/17621 [00:00<05:04, 57.54it/s]\u001b[A\n",
      "  1%|          | 118/17621 [00:00<03:54, 74.75it/s]\u001b[A\n",
      "  1%|          | 151/17621 [00:01<03:00, 96.93it/s]\u001b[A\n",
      "  1%|          | 184/17621 [00:01<02:22, 121.99it/s]\u001b[A\n",
      "  1%|          | 220/17621 [00:01<01:54, 152.01it/s]\u001b[A\n",
      "  1%|▏         | 263/17621 [00:01<01:32, 187.91it/s]\u001b[A\n",
      "  2%|▏         | 305/17621 [00:01<01:17, 224.46it/s]\u001b[A\n",
      "  2%|▏         | 341/17621 [00:01<01:08, 252.66it/s]\u001b[A\n",
      "  2%|▏         | 378/17621 [00:01<01:02, 277.00it/s]\u001b[A\n",
      "  2%|▏         | 428/17621 [00:01<00:53, 318.46it/s]\u001b[A\n",
      "  3%|▎         | 468/17621 [00:01<00:50, 338.55it/s]\u001b[A\n",
      "  3%|▎         | 512/17621 [00:02<00:47, 363.47it/s]\u001b[A\n",
      "  3%|▎         | 553/17621 [00:02<00:46, 367.46it/s]\u001b[A\n",
      "  3%|▎         | 612/17621 [00:02<00:41, 413.90it/s]\u001b[A\n",
      "  4%|▍         | 678/17621 [00:02<00:36, 465.03it/s]\u001b[A\n",
      "  4%|▍         | 734/17621 [00:02<00:34, 482.69it/s]\u001b[A\n",
      "  5%|▍         | 802/17621 [00:02<00:31, 527.54it/s]\u001b[A\n",
      "  5%|▍         | 863/17621 [00:02<00:30, 541.78it/s]\u001b[A\n",
      "  5%|▌         | 936/17621 [00:02<00:28, 584.57it/s]\u001b[A\n",
      "  6%|▌         | 998/17621 [00:02<00:31, 521.57it/s]\u001b[A\n",
      "  6%|▌         | 1054/17621 [00:03<00:34, 476.78it/s]\u001b[A\n",
      "  6%|▋         | 1105/17621 [00:03<00:40, 408.40it/s]\u001b[A\n",
      "  7%|▋         | 1152/17621 [00:03<00:39, 421.49it/s]\u001b[A\n",
      "  7%|▋         | 1215/17621 [00:03<00:35, 467.36it/s]\u001b[A\n",
      "  7%|▋         | 1266/17621 [00:03<00:36, 451.05it/s]\u001b[A\n",
      "  7%|▋         | 1314/17621 [00:03<00:42, 381.12it/s]\u001b[A\n",
      "  8%|▊         | 1375/17621 [00:03<00:37, 429.21it/s]\u001b[A\n",
      "  8%|▊         | 1446/17621 [00:03<00:33, 486.63it/s]\u001b[A\n",
      "  9%|▊         | 1520/17621 [00:03<00:29, 541.95it/s]\u001b[A\n",
      "  9%|▉         | 1589/17621 [00:04<00:27, 576.55it/s]\u001b[A\n",
      "  9%|▉         | 1652/17621 [00:04<00:27, 585.08it/s]\u001b[A\n",
      " 10%|▉         | 1717/17621 [00:04<00:26, 601.97it/s]\u001b[A\n",
      " 10%|█         | 1780/17621 [00:04<00:26, 593.88it/s]\u001b[A\n",
      " 11%|█         | 1856/17621 [00:04<00:24, 634.65it/s]\u001b[A\n",
      " 11%|█         | 1922/17621 [00:04<00:25, 627.22it/s]\u001b[A\n",
      " 11%|█▏        | 1998/17621 [00:04<00:23, 661.04it/s]\u001b[A\n",
      " 12%|█▏        | 2066/17621 [00:04<00:23, 665.23it/s]\u001b[A\n",
      " 12%|█▏        | 2134/17621 [00:04<00:23, 662.61it/s]\u001b[A\n",
      " 13%|█▎        | 2211/17621 [00:05<00:22, 689.20it/s]\u001b[A\n",
      " 13%|█▎        | 2304/17621 [00:05<00:20, 747.11it/s]\u001b[A\n",
      " 14%|█▎        | 2381/17621 [00:05<00:20, 743.36it/s]\u001b[A\n",
      " 14%|█▍        | 2457/17621 [00:05<00:22, 685.25it/s]\u001b[A\n",
      " 14%|█▍        | 2531/17621 [00:05<00:21, 698.20it/s]\u001b[A\n",
      " 15%|█▍        | 2636/17621 [00:05<00:19, 770.23it/s]\u001b[A\n",
      " 15%|█▌        | 2717/17621 [00:05<00:20, 740.90it/s]\u001b[A\n",
      " 16%|█▌        | 2797/17621 [00:05<00:19, 750.86it/s]\u001b[A\n",
      " 16%|█▋        | 2888/17621 [00:05<00:18, 791.75it/s]\u001b[A\n",
      " 17%|█▋        | 2970/17621 [00:06<00:20, 710.31it/s]\u001b[A\n",
      " 17%|█▋        | 3044/17621 [00:06<00:21, 668.29it/s]\u001b[A\n",
      " 18%|█▊        | 3131/17621 [00:06<00:20, 715.09it/s]\u001b[A\n",
      " 18%|█▊        | 3224/17621 [00:06<00:18, 767.49it/s]\u001b[A\n",
      " 19%|█▉        | 3334/17621 [00:06<00:16, 841.91it/s]\u001b[A\n",
      " 19%|█▉        | 3423/17621 [00:06<00:20, 707.89it/s]\u001b[A\n",
      " 20%|█▉        | 3501/17621 [00:06<00:20, 675.46it/s]\u001b[A\n",
      " 20%|██        | 3587/17621 [00:06<00:19, 721.87it/s]\u001b[A\n",
      " 21%|██        | 3701/17621 [00:06<00:17, 807.96it/s]\u001b[A\n",
      " 22%|██▏       | 3795/17621 [00:07<00:16, 842.91it/s]\u001b[A\n",
      " 22%|██▏       | 3885/17621 [00:07<00:16, 850.79it/s]\u001b[A\n",
      " 23%|██▎       | 3974/17621 [00:07<00:16, 851.86it/s]\u001b[A\n",
      " 23%|██▎       | 4070/17621 [00:07<00:15, 874.69it/s]\u001b[A\n",
      " 24%|██▎       | 4160/17621 [00:07<00:15, 867.19it/s]\u001b[A\n",
      " 24%|██▍       | 4249/17621 [00:07<00:15, 855.10it/s]\u001b[A\n",
      " 25%|██▍       | 4336/17621 [00:07<00:16, 787.68it/s]\u001b[A\n",
      " 25%|██▌       | 4445/17621 [00:07<00:15, 858.65it/s]\u001b[A\n",
      " 26%|██▌       | 4534/17621 [00:07<00:17, 750.63it/s]\u001b[A\n",
      " 26%|██▌       | 4614/17621 [00:08<00:17, 750.34it/s]\u001b[A\n",
      " 27%|██▋       | 4693/17621 [00:08<00:18, 685.66it/s]\u001b[A\n",
      " 27%|██▋       | 4765/17621 [00:08<00:18, 677.76it/s]\u001b[A\n",
      " 28%|██▊       | 4862/17621 [00:08<00:17, 742.05it/s]\u001b[A\n",
      " 28%|██▊       | 4978/17621 [00:08<00:15, 823.41it/s]\u001b[A\n",
      " 29%|██▉       | 5085/17621 [00:08<00:14, 883.31it/s]\u001b[A\n",
      " 29%|██▉       | 5192/17621 [00:08<00:13, 927.01it/s]\u001b[A\n",
      " 30%|███       | 5289/17621 [00:08<00:13, 936.00it/s]\u001b[A\n",
      " 31%|███       | 5386/17621 [00:08<00:14, 850.46it/s]\u001b[A\n",
      " 31%|███       | 5475/17621 [00:09<00:15, 806.75it/s]\u001b[A\n",
      " 32%|███▏      | 5571/17621 [00:09<00:14, 846.54it/s]\u001b[A\n",
      " 32%|███▏      | 5673/17621 [00:09<00:13, 890.34it/s]\u001b[A\n",
      " 33%|███▎      | 5789/17621 [00:09<00:12, 956.59it/s]\u001b[A\n",
      " 33%|███▎      | 5896/17621 [00:09<00:11, 985.66it/s]\u001b[A\n",
      " 34%|███▍      | 6008/17621 [00:09<00:11, 1022.00it/s]\u001b[A\n",
      " 35%|███▍      | 6113/17621 [00:09<00:11, 996.66it/s] \u001b[A\n",
      " 35%|███▌      | 6215/17621 [00:09<00:11, 995.58it/s]\u001b[A\n",
      " 36%|███▌      | 6316/17621 [00:09<00:11, 949.87it/s]\u001b[A\n",
      " 36%|███▋      | 6426/17621 [00:10<00:11, 989.22it/s]\u001b[A\n",
      " 37%|███▋      | 6563/17621 [00:10<00:10, 1078.49it/s]\u001b[A\n",
      " 38%|███▊      | 6675/17621 [00:10<00:10, 1001.46it/s]\u001b[A\n",
      " 38%|███▊      | 6779/17621 [00:10<00:11, 930.48it/s] \u001b[A\n",
      " 39%|███▉      | 6876/17621 [00:10<00:12, 864.13it/s]\u001b[A\n",
      " 40%|███▉      | 6966/17621 [00:10<00:13, 801.92it/s]\u001b[A\n",
      " 40%|████      | 7090/17621 [00:10<00:11, 896.98it/s]\u001b[A\n",
      " 41%|████      | 7186/17621 [00:10<00:11, 913.71it/s]\u001b[A\n",
      " 42%|████▏     | 7319/17621 [00:10<00:10, 1008.31it/s]\u001b[A\n",
      " 42%|████▏     | 7426/17621 [00:11<00:10, 1010.92it/s]\u001b[A\n",
      " 43%|████▎     | 7532/17621 [00:11<00:09, 1024.81it/s]\u001b[A\n",
      " 43%|████▎     | 7638/17621 [00:11<00:09, 1031.51it/s]\u001b[A\n",
      " 44%|████▍     | 7744/17621 [00:11<00:09, 1039.05it/s]\u001b[A\n",
      " 45%|████▍     | 7853/17621 [00:11<00:09, 1052.98it/s]\u001b[A\n",
      " 45%|████▌     | 7966/17621 [00:11<00:09, 1067.42it/s]\u001b[A\n",
      " 46%|████▌     | 8074/17621 [00:11<00:09, 1010.49it/s]\u001b[A\n",
      " 47%|████▋     | 8205/17621 [00:11<00:08, 1084.36it/s]\u001b[A\n",
      " 47%|████▋     | 8327/17621 [00:11<00:08, 1114.07it/s]\u001b[A\n",
      " 48%|████▊     | 8461/17621 [00:11<00:07, 1172.83it/s]\u001b[A\n",
      " 49%|████▊     | 8581/17621 [00:12<00:07, 1180.19it/s]\u001b[A\n",
      " 49%|████▉     | 8701/17621 [00:12<00:07, 1161.32it/s]\u001b[A\n",
      " 50%|█████     | 8836/17621 [00:12<00:07, 1208.86it/s]\u001b[A\n",
      " 51%|█████     | 8969/17621 [00:12<00:06, 1241.10it/s]\u001b[A\n",
      " 52%|█████▏    | 9095/17621 [00:12<00:08, 1012.50it/s]\u001b[A\n",
      " 52%|█████▏    | 9204/17621 [00:12<00:08, 974.88it/s] \u001b[A\n",
      " 53%|█████▎    | 9316/17621 [00:12<00:08, 1014.16it/s]\u001b[A\n",
      " 54%|█████▎    | 9450/17621 [00:12<00:07, 1084.24it/s]\u001b[A\n",
      " 54%|█████▍    | 9578/17621 [00:13<00:07, 1135.47it/s]\u001b[A\n",
      " 55%|█████▌    | 9696/17621 [00:13<00:07, 1044.73it/s]\u001b[A\n",
      " 56%|█████▌    | 9805/17621 [00:13<00:07, 1025.71it/s]\u001b[A\n",
      " 56%|█████▌    | 9911/17621 [00:13<00:07, 1029.09it/s]\u001b[A\n",
      " 57%|█████▋    | 10019/17621 [00:13<00:07, 1043.28it/s]\u001b[A\n",
      " 58%|█████▊    | 10145/17621 [00:13<00:06, 1096.74it/s]\u001b[A\n",
      " 58%|█████▊    | 10257/17621 [00:13<00:06, 1085.37it/s]\u001b[A\n",
      " 59%|█████▉    | 10378/17621 [00:13<00:06, 1113.84it/s]\u001b[A\n",
      " 60%|█████▉    | 10508/17621 [00:13<00:06, 1157.87it/s]\u001b[A\n",
      " 60%|██████    | 10631/17621 [00:13<00:05, 1169.46it/s]\u001b[A\n",
      " 61%|██████    | 10749/17621 [00:14<00:06, 1144.20it/s]\u001b[A\n",
      " 62%|██████▏   | 10865/17621 [00:14<00:06, 1025.18it/s]\u001b[A\n",
      " 62%|██████▏   | 10980/17621 [00:14<00:06, 1059.25it/s]\u001b[A\n",
      " 63%|██████▎   | 11112/17621 [00:14<00:05, 1125.52it/s]\u001b[A\n",
      " 64%|██████▍   | 11239/17621 [00:14<00:05, 1157.83it/s]\u001b[A\n",
      " 64%|██████▍   | 11357/17621 [00:14<00:05, 1107.23it/s]\u001b[A\n",
      " 65%|██████▌   | 11502/17621 [00:14<00:05, 1183.33it/s]\u001b[A\n",
      " 66%|██████▌   | 11626/17621 [00:14<00:05, 1195.61it/s]\u001b[A\n",
      " 67%|██████▋   | 11748/17621 [00:14<00:04, 1183.42it/s]\u001b[A\n",
      " 67%|██████▋   | 11868/17621 [00:15<00:05, 1101.78it/s]\u001b[A\n",
      " 68%|██████▊   | 11981/17621 [00:15<00:05, 1073.96it/s]\u001b[A\n",
      " 69%|██████▊   | 12091/17621 [00:15<00:05, 1026.40it/s]\u001b[A\n",
      " 69%|██████▉   | 12203/17621 [00:15<00:05, 1052.78it/s]\u001b[A\n",
      " 70%|██████▉   | 12310/17621 [00:15<00:05, 1048.93it/s]\u001b[A\n",
      " 71%|███████   | 12450/17621 [00:15<00:04, 1133.11it/s]\u001b[A\n",
      " 71%|███████▏  | 12566/17621 [00:15<00:04, 1116.95it/s]\u001b[A\n",
      " 72%|███████▏  | 12687/17621 [00:15<00:04, 1140.82it/s]\u001b[A\n",
      " 73%|███████▎  | 12853/17621 [00:15<00:03, 1257.41it/s]\u001b[A\n",
      " 74%|███████▎  | 12984/17621 [00:16<00:03, 1244.46it/s]\u001b[A\n",
      " 74%|███████▍  | 13112/17621 [00:16<00:03, 1253.21it/s]\u001b[A\n",
      " 75%|███████▌  | 13245/17621 [00:16<00:03, 1274.15it/s]\u001b[A\n",
      " 76%|███████▌  | 13375/17621 [00:16<00:03, 1270.21it/s]\u001b[A\n",
      " 77%|███████▋  | 13504/17621 [00:16<00:03, 1266.69it/s]\u001b[A\n",
      " 77%|███████▋  | 13632/17621 [00:16<00:03, 1257.48it/s]\u001b[A\n",
      " 78%|███████▊  | 13759/17621 [00:16<00:03, 1207.56it/s]\u001b[A\n",
      " 79%|███████▉  | 13881/17621 [00:16<00:03, 1054.41it/s]\u001b[A\n",
      " 79%|███████▉  | 13991/17621 [00:16<00:03, 1031.39it/s]\u001b[A\n",
      " 80%|████████  | 14098/17621 [00:17<00:03, 1026.70it/s]\u001b[A\n",
      " 81%|████████  | 14212/17621 [00:17<00:03, 1056.92it/s]\u001b[A\n",
      " 81%|████████▏ | 14336/17621 [00:17<00:02, 1103.00it/s]\u001b[A\n",
      " 82%|████████▏ | 14449/17621 [00:17<00:02, 1108.15it/s]\u001b[A\n",
      " 83%|████████▎ | 14562/17621 [00:17<00:03, 992.47it/s] \u001b[A\n",
      " 83%|████████▎ | 14665/17621 [00:17<00:03, 854.84it/s]\u001b[A\n",
      " 84%|████████▍ | 14772/17621 [00:17<00:03, 909.02it/s]\u001b[A\n",
      " 84%|████████▍ | 14887/17621 [00:17<00:02, 969.57it/s]\u001b[A\n",
      " 85%|████████▌ | 15021/17621 [00:17<00:02, 1056.53it/s]\u001b[A\n",
      " 86%|████████▌ | 15133/17621 [00:18<00:02, 1028.30it/s]\u001b[A\n",
      " 87%|████████▋ | 15261/17621 [00:18<00:02, 1092.15it/s]\u001b[A\n",
      " 87%|████████▋ | 15378/17621 [00:18<00:02, 1111.78it/s]\u001b[A\n",
      " 88%|████████▊ | 15506/17621 [00:18<00:01, 1137.69it/s]\u001b[A\n",
      " 89%|████████▉ | 15642/17621 [00:18<00:01, 1195.05it/s]\u001b[A\n",
      " 90%|████████▉ | 15782/17621 [00:18<00:01, 1246.91it/s]\u001b[A\n",
      " 90%|█████████ | 15917/17621 [00:18<00:01, 1275.94it/s]\u001b[A\n",
      " 91%|█████████ | 16048/17621 [00:18<00:01, 1285.23it/s]\u001b[A\n",
      " 92%|█████████▏| 16190/17621 [00:18<00:01, 1322.43it/s]\u001b[A\n",
      " 93%|█████████▎| 16324/17621 [00:18<00:01, 1293.60it/s]\u001b[A\n",
      " 93%|█████████▎| 16455/17621 [00:19<00:00, 1291.32it/s]\u001b[A\n",
      " 94%|█████████▍| 16592/17621 [00:19<00:00, 1309.97it/s]\u001b[A\n",
      " 95%|█████████▍| 16724/17621 [00:19<00:00, 1117.38it/s]\u001b[A\n",
      " 96%|█████████▌| 16854/17621 [00:19<00:00, 1159.49it/s]\u001b[A\n",
      " 96%|█████████▋| 16975/17621 [00:19<00:00, 1171.47it/s]\u001b[A\n",
      " 97%|█████████▋| 17096/17621 [00:19<00:00, 1112.98it/s]\u001b[A\n",
      " 98%|█████████▊| 17210/17621 [00:19<00:00, 962.79it/s] \u001b[A\n",
      " 98%|█████████▊| 17312/17621 [00:19<00:00, 916.28it/s]\u001b[A\n",
      " 99%|█████████▉| 17453/17621 [00:20<00:00, 1023.70it/s]\u001b[A\n",
      "100%|██████████| 17621/17621 [00:20<00:00, 874.96it/s] \u001b[A\n",
      "\n",
      "  0%|          | 0/1958 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|▋         | 124/1958 [00:00<00:01, 1236.81it/s]\u001b[A\n",
      " 14%|█▍        | 272/1958 [00:00<00:01, 1300.00it/s]\u001b[A\n",
      " 22%|██▏       | 432/1958 [00:00<00:01, 1375.77it/s]\u001b[A\n",
      " 32%|███▏      | 633/1958 [00:00<00:00, 1518.41it/s]\u001b[A\n",
      " 42%|████▏     | 827/1958 [00:00<00:00, 1618.65it/s]\u001b[A\n",
      " 52%|█████▏    | 1025/1958 [00:00<00:00, 1705.07it/s]\u001b[A\n",
      " 62%|██████▏   | 1217/1958 [00:00<00:00, 1761.81it/s]\u001b[A\n",
      " 73%|███████▎  | 1429/1958 [00:00<00:00, 1846.14it/s]\u001b[A\n",
      " 82%|████████▏ | 1611/1958 [00:00<00:00, 1816.40it/s]\u001b[A\n",
      "100%|██████████| 1958/1958 [00:01<00:00, 1787.60it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'xgboost' has no attribute 'XGBClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e31f3c03cce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fitting a simple xgboost on glove features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxvalid_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'xgboost' has no attribute 'XGBClassifier'"
     ]
    }
   ],
   "source": [
    "# Let's see the performance of xgboost on glove features:\n",
    "\n",
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we see that a simple tuning of parameters can improve xgboost score on GloVe features! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "# We shall train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first:\n",
    "\n",
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "17621/17621 [==============================] - 3s 179us/step - loss: 0.9472 - val_loss: 0.7336\n",
      "Epoch 2/5\n",
      "17621/17621 [==============================] - 3s 144us/step - loss: 0.7146 - val_loss: 0.6999\n",
      "Epoch 3/5\n",
      "17621/17621 [==============================] - 2s 101us/step - loss: 0.6590 - val_loss: 0.6872\n",
      "Epoch 4/5\n",
      "17621/17621 [==============================] - 2s 126us/step - loss: 0.6055 - val_loss: 0.6968\n",
      "Epoch 5/5\n",
      "17621/17621 [==============================] - 2s 107us/step - loss: 0.5657 - val_loss: 0.6736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16a514390>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to keep on tuning the parameters of the neural network, add more layers, increase dropout to get better \n",
    "# results. Here, I'm just showing that its fast to implement and run and gets better result than xgboost without any optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/25943 [00:00<?, ?it/s]\u001b[A\n",
      " 22%|██▏       | 5684/25943 [00:00<00:00, 56839.65it/s]\u001b[A\n",
      " 37%|███▋      | 9684/25943 [00:00<00:00, 50348.19it/s]\u001b[A\n",
      " 59%|█████▉    | 15322/25943 [00:00<00:00, 51927.44it/s]\u001b[A\n",
      "100%|██████████| 25943/25943 [00:00<00:00, 59091.68it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/100\n",
      "17621/17621 [==============================] - 91s 5ms/step - loss: 1.0964 - val_loss: 0.9890\n",
      "Epoch 2/100\n",
      "17621/17621 [==============================] - 116s 7ms/step - loss: 0.9636 - val_loss: 0.8640\n",
      "Epoch 3/100\n",
      "17621/17621 [==============================] - 94s 5ms/step - loss: 0.8968 - val_loss: 0.7978\n",
      "Epoch 4/100\n",
      "17621/17621 [==============================] - 91s 5ms/step - loss: 0.8582 - val_loss: 0.7832\n",
      "Epoch 5/100\n",
      "17621/17621 [==============================] - 90s 5ms/step - loss: 0.8303 - val_loss: 0.7427\n",
      "Epoch 6/100\n",
      "17621/17621 [==============================] - 89s 5ms/step - loss: 0.8128 - val_loss: 0.7336\n",
      "Epoch 7/100\n",
      "17621/17621 [==============================] - 102s 6ms/step - loss: 0.7884 - val_loss: 0.7168\n",
      "Epoch 8/100\n",
      "17621/17621 [==============================] - 107s 6ms/step - loss: 0.7647 - val_loss: 0.6935\n",
      "Epoch 9/100\n",
      "17621/17621 [==============================] - 101s 6ms/step - loss: 0.7428 - val_loss: 0.6811\n",
      "Epoch 10/100\n",
      "17621/17621 [==============================] - 101s 6ms/step - loss: 0.7335 - val_loss: 0.6709\n",
      "Epoch 11/100\n",
      "17621/17621 [==============================] - 94s 5ms/step - loss: 0.7110 - val_loss: 0.6463\n",
      "Epoch 12/100\n",
      "10240/17621 [================>.............] - ETA: 35s - loss: 0.6877"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-47731ed843aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n\u001b[0;32m---> 26\u001b[0;31m           verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# How do I use early stopping?\n",
    "\n",
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is it ok to use so much dropout? Well, fit the model with no or little dropout and you will see that it starts to \n",
    "overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bi-directional LSTM using Keras\n",
    "\n",
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try two layers of GRU:\n",
    "\n",
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
